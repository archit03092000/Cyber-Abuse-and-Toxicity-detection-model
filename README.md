Dealing with toxicity online and curbing harassment has been a growing problem since social  media and online conversations have become a part of our everyday life. It is almost impossible  to engage in online conversations without witnessing toxic behavior like unwarranted  harassment or disrespect. The digital world has the potential of becoming a community that fosters growth, sympathy, and education by learning from people, but is hindered by users who  take advantage of this in-person disconnect. Social media platforms, online news commenting  spaces, and many other public forum of the Internet have become increasingly known for issues  of abusive behavior such as cyberbullying and personal attacks. These types of comments  have given rise to severe anxiety problems and depression among the population of different  age groups interacting on these social media platforms.  
Dealing with such online toxicity, harassment is the new challenge waiting ahead of us with  the rapid growth of this era of social media. However, determining whether a comment or post  should be “flagged” or not is still difficult and time-consuming, and many platforms are still  searching for more efficient moderation solutions. Automating the process of identifying abuse  in comments would not only save websites time but would also increase user safety and  improve discussions online 
The objective of our project is to develop an NLP based AI/ML application that is capable of  automatically identifying and filtering out abusive/toxic behavior such as hate speech, harassment,  personal abuse, racism, threats, insults, bullying, etc. from tweets/comments on social media platforms to ensure a safe and secure environment for the users
