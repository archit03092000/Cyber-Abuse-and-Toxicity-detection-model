# -*- coding: utf-8 -*-
"""OnlineAbuseIdentification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LkHFf4Q6QBK9SElJLyRQYCz0A6S4R3NF
"""

!pip install nltk

import matplotlib.pyplot as plt
import nltk
import numpy as np
import re 
import pandas as pd
import string
import seaborn as sns

from nltk.corpus import stopwords  
from nltk.stem.lancaster import LancasterStemmer  


from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

nltk.download('stopwords')

nltk.download('wordnet')

set(stopwords.words('english'))

data = pd.read_csv('/content/drive/MyDrive/MINOR2/train.csv')
data.head()

data_test = pd.read_csv('/content/drive/MyDrive/MINOR2/test.csv')
data_test.head()

data_testlabel = pd.read_csv('/content/drive/MyDrive/MINOR2/test_labels.csv')
data_testlabel.head()

data_testlabel=data_testlabel[data_testlabel['toxic']!=-1]
data_testlabel

extra_data=pd.merge(data_test,data_testlabel,on="id")
extra_data

new_data=pd.concat([data,extra_data],ignore_index=True)
new_data

data=new_data

data.info()

data.describe()

data.isnull().sum()

print(data.corr())

print(sns.heatmap(data.corr()))

# checking the skewness for the features:
data.skew()

data_count=data.iloc[:,2:].sum()

data_count

plt.figure(figsize=(8,4))


ax = sns.barplot(data_count.index, data_count.values, alpha=0.8)

plt.title("No. of comments per class")
plt.ylabel('No. of Occurrences', fontsize=12)
plt.xlabel('Type ', fontsize=12)

rects = ax.patches
labels = data_count.values
for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

cols_target = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']
df_distribution = data[cols_target].sum()\
                            .to_frame()\
                            .rename(columns={0: 'count'})\
                            .sort_values('count')

df_distribution.plot.pie(y='count',
                                      title='Label distribution over comments',
                                      figsize=(5, 5))\
                            .legend(loc='center left', bbox_to_anchor=(1.3, 0.5))

# Create a bar graph 
num_rows = len(data)
sum_tox = data['toxic'].sum() / num_rows * 100
sum_sev = data['severe_toxic'].sum() / num_rows * 100
sum_obs = data['obscene'].sum() / num_rows * 100
sum_thr = data['threat'].sum() / num_rows * 100
sum_ins = data['insult'].sum() / num_rows * 100
sum_ide = data['identity_hate'].sum() / num_rows * 100


ind = np.arange(6)

ax = plt.barh(ind, [sum_tox, sum_obs, sum_ins, sum_sev, sum_ide, sum_thr])
plt.xlabel('Percentage (%)', size=20)
plt.xticks(np.arange(0, 30, 5), size=20)
plt.title('% of comments in various categories', size=22)
plt.yticks(ind, ('Toxic', 'Obscene', 'Insult', 'Severe Toxic', 'Identity Hate', 'Threat', ), size=15)

plt.gca().invert_yaxis()
plt.show()

#text before preprocessing
data['comment_text'][0]

# Text preprocessing steps - remove numbers, capital letters, punctuation, '\n'
import re
import string


# remove all numbers with letters attached to them
alphanumeric = lambda x: re.sub('\w*\d\w*', ' ', x)

# '[%s]' % re.escape(string.punctuation),' ' - replace punctuation with white space
# .lower() - convert all strings to lowercase 
punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())

# Remove all '\n' in the string and replace it with a space
remove_n = lambda x: re.sub("\n", " ", x)

# Remove all non-ascii characters 
remove_non_ascii = lambda x: re.sub(r'[^\x00-\x7f]',r' ', x)

# Apply all the lambda functions wrote previously through .map on the comments column
data['comment_text'] = data['comment_text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)

data['comment_text'][0]

from nltk.stem import WordNetLemmatizer
# Replace email addresses with 'email'
data['comment_text'] = data['comment_text'].str.replace(r'^.+@[^\.].*\.[a-z]{2,}$',
                                 'emailaddress')

# Replace URLs with 'webaddress'
data['comment_text'] = data['comment_text'].str.replace(r'^http\://[a-zA-Z0-9\-\.]+\.[a-zA-Z]{2,3}(/\S*)?$',
                                  'webaddress')

# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)
data['comment_text'] = data['comment_text'].str.replace(r'£|\$', 'dollers')
    
# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'
data['comment_text'] = data['comment_text'].str.replace(r'^\(?[\d]{3}\)?[\s-]?[\d]{3}[\s-]?[\d]{4}$',
                                  'phonenumber')

    
# Replace numbers with 'numbr'
data['comment_text'] = data['comment_text'].str.replace(r'\d+(\.\d+)?', 'numbr')


data['comment_text'] = data['comment_text'].apply(lambda x: ' '.join(
    term for term in x.split() if term not in string.punctuation))
stop_words = set(stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure'])
data['comment_text'] = data['comment_text'].apply(lambda x: ' '.join(
    term for term in x.split() if term not in stop_words))

lem=WordNetLemmatizer()
data['comment_text'] = data['comment_text'].apply(lambda x: ' '.join(
 lem.lemmatize(t) for t in x.split()))
data['comment_text'][0]

data_tox = data.loc[:,['id','comment_text','toxic']]
data_sev = data.loc[:,['id','comment_text','severe_toxic']]
data_obs = data.loc[:,['id','comment_text','obscene']]
data_thr = data.loc[:,['id','comment_text','threat']]
data_ins = data.loc[:,['id','comment_text','insult']]
data_ide = data.loc[:,['id','comment_text','identity_hate']]

import wordcloud
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

def wordcloud(df, label):
    
    subset=df[df[label]==1]
    text=subset.comment_text.values
    wc= WordCloud(background_color="black",max_words=4000)

    wc.generate(" ".join(text))

    plt.figure(figsize=(20,20))
    plt.subplot(221)
    plt.axis("off")
    plt.title("Words frequented in {}".format(label), fontsize=20)
    plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)

wordcloud(data_ide,'identity_hate')

wordcloud(data_ins, 'insult')

wordcloud(data_sev, 'severe_toxic')

wordcloud(data_obs, 'obscene')

wordcloud(data_tox, 'toxic')

wordcloud(data_thr, 'threat')



data_tox_1 = data_tox[data_tox['toxic'] == 1].iloc[:,:]
data_tox_1.shape

data_tox_0 = data_tox[data_tox['toxic'] == 0].iloc[0:22000,:]

data_tox_done = pd.concat([data_tox_1, data_tox_0], axis=0)
data_tox_done.shape

data_sev[data_sev['severe_toxic'] == 1].count()

data_sev_1 = data_sev[data_sev['severe_toxic'] == 1].iloc[:,:]
data_sev_0 = data_sev[data_sev['severe_toxic'] == 0].iloc[0:4500,:]
data_sev_done = pd.concat([data_sev_1, data_sev_0], axis=0)
data_sev_done.shape

data_obs[data_obs['obscene'] == 1].count()

data_obs_1 = data_obs[data_obs['obscene'] == 1].iloc[:,:]
data_obs_0 = data_obs[data_obs['obscene'] == 0].iloc[0:12500,:]
data_obs_done = pd.concat([data_obs_1, data_obs_0], axis=0)
data_obs_done.shape

data_thr[data_thr['threat'] == 1].count()

data_thr_1 = data_thr[data_thr['threat'] == 1].iloc[:,:]

data_thr_0 = data_thr[data_thr['threat'] == 0].iloc[0:1912,:]  
data_thr_done = pd.concat([data_thr_1, data_thr_0], axis=0)
data_thr_done.shape

data_ins[data_ins['insult'] == 1].count()

data_ins_1 = data_ins[data_ins['insult'] == 1].iloc[:,:]
data_ins_0 = data_ins[data_ins['insult'] == 0].iloc[0:11500,:]
data_ins_done = pd.concat([data_ins_1, data_ins_0], axis=0)
data_ins_done.shape

data_ide[data_ide['identity_hate'] == 1].count()

data_ide_1 = data_ide[data_ide['identity_hate'] == 1].iloc[:,:] # 20%
data_ide_0 = data_ide[data_ide['identity_hate'] == 0].iloc[0:5620,:] # 80%
data_ide_done = pd.concat([data_ide_1, data_ide_0], axis=0)
data_ide_done.shape



from sklearn import preprocessing
from sklearn.feature_selection import SelectFromModel

# Import tools to split data and evaluate model performance
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve, fbeta_score, confusion_matrix
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.metrics import accuracy_score

# Import ML algos
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier

def cv_tf_train_test(df_done,label,vectorizer,ngram):

    
    X = df_done.comment_text
    y = df_done[label]
 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    cv1 = vectorizer(ngram_range=(ngram), stop_words='english')
    
    X_train_cv1 = cv1.fit_transform(X_train)
    X_test_cv1  = cv1.transform(X_test)    
    
        
    lr = LogisticRegression()
    lr.fit(X_train_cv1, y_train)
    print('lr done')
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train_cv1, y_train)
    print('knn done')

    bnb = BernoulliNB()
    bnb.fit(X_train_cv1, y_train)
    print('bnb done')
    
    mnb = MultinomialNB()
    mnb.fit(X_train_cv1, y_train)
    print('mnb done')
    
    svm_model = LinearSVC()
    svm_model.fit(X_train_cv1, y_train)
    print('svm done')

    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)
    randomforest.fit(X_train_cv1, y_train)
    print('rdf done')
    
    
    f1_score_data = {'F1 Score':[f1_score(lr.predict(X_test_cv1), y_test), f1_score(knn.predict(X_test_cv1), y_test), 
                                f1_score(bnb.predict(X_test_cv1), y_test), f1_score(mnb.predict(X_test_cv1), y_test),
                                f1_score(svm_model.predict(X_test_cv1), y_test), f1_score(randomforest.predict(X_test_cv1), y_test)]} 

    acc_score_data = {'Accuracy Score':[accuracy_score(lr.predict(X_test_cv1), y_test), accuracy_score(knn.predict(X_test_cv1), y_test), 
                                accuracy_score(bnb.predict(X_test_cv1), y_test), accuracy_score(mnb.predict(X_test_cv1), y_test),
                                accuracy_score(svm_model.predict(X_test_cv1), y_test), accuracy_score(randomforest.predict(X_test_cv1), y_test)]} 
                          
    
    df_f1 = pd.DataFrame(f1_score_data, index=['Log Regression','KNN', 'BernoulliNB', 'MultinomialNB', 'SVM', 'Random Forest'])
    df_f2 = pd.DataFrame(acc_score_data, index=['Log Regression','KNN', 'BernoulliNB', 'MultinomialNB', 'SVM', 'Random Forest'])    

    return [df_f1,df_f2]

df_tox_cv = cv_tf_train_test(data_tox_done, 'toxic', TfidfVectorizer, (1,1))
df_tox_cv[0].rename(columns={'F1 Score': 'F1 Score(toxic)'}, inplace=True)

df_tox_cv[0]

# Various permutations of the dataset, category, vectorizer and n-gram

# cv_tf_train_test(data_tox_done, 'toxic', CountVectorizer, (1,1))
# cv_tf_train_test(data_sev_done, 'severe_toxic', CountVectorizer, (1,1))
# cv_tf_train_test(data_obs_done, 'obscene', CountVectorizer, (1,1))
# cv_tf_train_test(data_thr_done, 'threat', CountVectorizer, (1,1))
# cv_tf_train_test(data_ins_done, 'insult', CountVectorizer, (1,1))
# cv_tf_train_test(data_ide_done, 'identity_hate', CountVectorizer, (1,1))

# cv_tf_train_test(data_tox_done, 'toxic', TfidfVectorizer, (1,1))
#cv_tf_train_test(data_sev_done, 'severe_toxic', TfidfVectorizer, (1,1))
# cv_tf_train_test(data_obs_done, 'obscene', TfidfVectorizer, (1,1))
# cv_tf_train_test(data_thr_done, 'threat', TfidfVectorizer, (1,1))
# cv_tf_train_test(data_ins_done, 'insult', TfidfVectorizer, (1,1))
#cv_tf_train_test(data_ide_done, 'identity_hate', TfidfVectorizer, (1,1))

df_tox_cv[1].rename(columns={'Accuracy Score': 'Accuracy Score(toxic)'}, inplace=True)
df_tox_cv[1]

df_sev_cv = cv_tf_train_test(data_sev_done, 'severe_toxic', TfidfVectorizer, (1,1))
df_sev_cv[0].rename(columns={'F1 Score': 'F1 Score(severe_toxic)'}, inplace=True)
df_sev_cv[0]

df_sev_cv[1].rename(columns={'Accuracy Score': 'Accuracy Score(severe_toxic)'}, inplace=True)
df_sev_cv[1]

df_obs_cv = cv_tf_train_test(data_obs_done, 'obscene', TfidfVectorizer, (1,1))
df_obs_cv[0].rename(columns={'F1 Score': 'F1 Score(obscene)'}, inplace=True)
df_obs_cv[0]

df_obs_cv[1].rename(columns={'Accuracy Score': 'Accuracy Score(obscene)'}, inplace=True)
df_obs_cv[1]

df_thr_cv = cv_tf_train_test(data_thr_done, 'threat', TfidfVectorizer, (1,1))
df_thr_cv[0].rename(columns={'F1 Score': 'F1 Score(threat)'}, inplace=True)
df_thr_cv[0]

df_thr_cv[1].rename(columns={'Accuracy Score': 'Accuracy Score(threat)'}, inplace=True)
df_thr_cv[1]

df_ins_cv = cv_tf_train_test(data_ins_done, 'insult', TfidfVectorizer, (1,1))
df_ins_cv[0].rename(columns={'F1 Score': 'F1 Score(insult)'}, inplace=True)
df_ins_cv[0]

df_ins_cv[1].rename(columns={'Accuracy Score': 'Accuracy Score(insult)'}, inplace=True)
df_ins_cv[1]

df_ide_cv = cv_tf_train_test(data_ide_done, 'identity_hate', TfidfVectorizer, (1,1))
df_ide_cv[0].rename(columns={'F1 Score': 'F1 Score(identity_hate)'}, inplace=True)
df_ide_cv[0]

df_ide_cv[1].rename(columns={'Accuracy Score': 'Accuracy Score(identity_hate)'}, inplace=True)
df_ide_cv[1]

f1_all = pd.concat([df_tox_cv[0], df_sev_cv[0], df_obs_cv[0], df_ins_cv[0], df_thr_cv[0], df_ide_cv[0]], axis=1)
f1_all_trp = f1_all.transpose()
f1_all_trp

acc_all = pd.concat([df_tox_cv[1], df_sev_cv[1], df_obs_cv[1], df_ins_cv[1], df_thr_cv[1], df_ide_cv[1]], axis=1)
acc_all_trp = acc_all.transpose()
acc_all_trp

sns.lineplot(data=f1_all_trp, markers=True)
plt.xticks(rotation='90', fontsize=10)
plt.yticks(fontsize=12)
plt.legend(loc='best')
plt.title('F1 Score of ML models', fontsize=20)

sns.lineplot(data=acc_all_trp, markers=True)
plt.xticks(rotation='90', fontsize=10)
plt.yticks(fontsize=12)
plt.legend(loc='best')
plt.title('Accuracy Score of ML models', fontsize=20)

X = data_tox_done.comment_text
y = data_tox_done['toxic']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')

X_train_fit = tfv.fit_transform(X_train)  
X_test_fit = tfv.transform(X_test)  

#randomforest = RandomForestClassifier(n_estimators=100, random_state=42)

#randomforest.fit(X_train_fit, y_train)
#randomforest.predict(X_test_fit)

comment1 = ["I don't give a fuck"]
comment2 = ['What is up garden apple doing']

comment1_vect = tfv.transform(comment1)
randomforest.predict_proba(comment1_vect)[:,1]

comment2_vect = tfv.transform(comment2)
randomforest.predict_proba(comment2_vect)[:,1]

from sklearn.calibration import CalibratedClassifierCV

svm_model = LinearSVC()
svm_model=CalibratedClassifierCV(svm_model)
svm_model.fit(X_train_fit, y_train)
svm_model.predict(X_test_fit)

accuracy_score(svm_model.predict(X_test_fit), y_test)

accuracy_score(svm_model.predict(X_train_fit), y_train)

#accuracy_score(randomforest.predict(X_test_fit), y_test)



comment1 = ["I don't give a hell about you"]
comment2 = ['What is up garden apple doing']

comment1_vect = tfv.transform(comment1)
svm_model.predict_proba(comment1_vect)[:,1]

comment2_vect = tfv.transform(comment2)
svm_model.predict_proba(comment2_vect)[:,1]

def train_model(df,label):
    X = df.comment_text
    y = df[label]

    # Split our data into training and test data 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    cv1 = TfidfVectorizer(ngram_range=(1,1), stop_words='english')
    
    X_train_fit = cv1.fit_transform(X_train)
    X_test_fit  = cv1.transform(X_test)
    svm_model = LinearSVC()
    svm_model=CalibratedClassifierCV(svm_model)
    svm_model.fit(X_train_fit, y_train)
    #svm_model.predict(X_test_fit)
    res=[accuracy_score(svm_model.predict(X_train_fit), y_train),accuracy_score(svm_model.predict(X_test_fit), y_test),f1_score(svm_model.predict(X_test_fit), y_test),precision_score(svm_model.predict(X_test_fit), y_test),recall_score(svm_model.predict(X_test_fit), y_test)]
    df_res=pd.DataFrame([res],index=[label],columns=["Train Accuracy","Test Accuracy","F1 Score","Precision","Recall"])
    #print (df_res)
    return df_res

datalist = [data_tox_done, data_sev_done, data_obs_done, data_ins_done, data_thr_done, data_ide_done]
label = ['toxic', 'severe_toxic', 'obscene', 'insult', 'threat', 'identity_hate']
df=pd.DataFrame()
for i,j in zip(datalist,label):
    #train_model(i, j)
    df=df.append(train_model(i, j))
df

import pickle

def pickle_model(df, label):
    
    X = df.comment_text
    y = df[label]

    tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')
    
    X_vect = tfv.fit_transform(X)  
    
    with open(r"{}.pkl".format(label + '_vect'), "wb") as f:   
        pickle.dump(tfv, f)   
        
    #randomforest = RandomForestClassifier(n_estimators=100, random_state=42)
    #randomforest.fit(X_vect, y)
    
    model = LinearSVC()
    model=CalibratedClassifierCV(model)
    model.fit(X_vect, y)
    
    with open(r"{}.pkl".format(label + '_model'), "wb") as f:  
        pickle.dump(model, f)

for i,j in zip(datalist,label):
    pickle_model(i, j)